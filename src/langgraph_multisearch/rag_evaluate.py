# langgraph_multisearch/rag_evaluate.py
import json
import time
import rag_logger as rl

from langgraph_multisearch.rag_state import RAGState
from llm_config import get_llm, log_usage

def _get_page_content(d):
    if isinstance(d, dict):
        return d.get("page_content", "")
    page = getattr(d, "page_content", "")
    return page

def _get_metadata(doc):
    if hasattr(doc, "metadata"):
        return doc.metadata
    if isinstance(doc, dict):
        return doc.get("metadata", {})
    return {}

def node_score(state: RAGState):
    """ë‹µë³€ í‰ê°€ + ìƒì„¸ ë¡œê¹…"""
    eval_start = time.time()
    
    answer = state["answer"]
    question = state["question"]
    docs = state["docs"]

    doc_text = "\n\n".join(
        f"[ë¬¸ì„œ {i+1}]\n{_get_page_content(d)[:1500]}"
        for i, d in enumerate(docs)
    )

    metadata_text = "\n\n".join(
        f"[ë¬¸ì„œ {i+1} ë©”íƒ€ë°ì´í„°]\n" + 
        "\n".join(f"{k}: {v}" for k, v in _get_metadata(d).items())
        for i, d in enumerate(docs)
    )

    prompt = f"""
    ë‹¹ì‹ ì€ ì§ˆë¬¸-ë‹µë³€ ì¼ì¹˜ë„ë¥¼ í‰ê°€í•˜ëŠ” ì „ë¬¸ ì‹¬ì‚¬ê´€ì…ë‹ˆë‹¤.

    â–£ ì§ˆë¬¸:
    {question}

    â–£ ë‹µë³€:
    {answer}

    â–£ ì°¸ê³  ë¬¸ì„œ ë‚´ìš©:
    {doc_text}

    â–£ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° (ë°œì£¼ê¸°ê´€/ì˜ˆì‚°/ë‚ ì§œ ë“±):
    {metadata_text}

    ì´ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ ì—„ê²©í•˜ê²Œ í‰ê°€í•˜ì„¸ìš”.

    í‰ê°€ ê¸°ì¤€:
    - ë‹µë³€ì´ ë¬¸ì„œ ë‚´ìš© ë˜ëŠ” í•´ë‹¹ ë¬¸ì„œì˜ ë©”íƒ€ë°ì´í„°ì— ê¸°ë°˜í–ˆëŠ”ê°€?
    - ë¬¸ì„œì— ì—†ëŠ” ìƒì„¸ ìˆ˜ì¹˜, ë‚ ì§œ, ì˜ˆì‚° ë“±ì„ ì¶”ì¸¡í–ˆëŠ”ê°€?
    - ë¬¸ì„œ ê·¼ê±°ì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ì •í™•íˆ ë°˜ì˜í–ˆëŠ”ê°€?

    JSON ONLY:
    {{
        "score": 0.0,
        "reason": "..."
    }}
    """
    
    judge_llm = get_llm('judge')
    judge_raw = judge_llm.invoke(prompt)
    judge_text = judge_raw.content if hasattr(judge_raw, "content") else str(judge_raw)

    print("[JUDGE RAW OUTPUT]\n", judge_text)

    try:
        cleaned = judge_text.replace('```json', '').replace('```', '').strip()
        parsed = json.loads(cleaned)
        score = float(parsed.get("score", 0.0))
        reason = parsed.get("reason", "")
        parse_success = True
    except Exception as e:
        print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
        score = 0.0
        reason = "JSON íŒŒì‹± ì‹¤íŒ¨"
        parse_success = False

    eval_time = time.time() - eval_start
    
    # í† í° + ë¹„ìš© ë¡œê¹…
    log_usage('judge', judge_raw)
    
    # ğŸ”¥ í‰ê°€ í†µê³„ ë¡œê¹…
    rl.log_evaluation({
        "evaluation/time_sec": eval_time,
        "evaluation/score": score,
        "evaluation/reason": reason[:100],  # ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
        "evaluation/reason_length": len(reason),
        "evaluation/parse_success": parse_success,
        "evaluation/prompt_length": len(prompt),
        "evaluation/num_docs_evaluated": len(docs),
        "evaluation/is_compare": state.get("is_compare", False)
    })

    return {
        "score": score, 
        "evaluate_reason": reason
    }