"""OpenAI ì„ë² ë”©ì„ ì‚¬ìš©í•´ JSON íŒŒì¼ë¡œ ê´€ë¦¬í•˜ëŠ” ê°„ë‹¨í•œ ë²¡í„°ìŠ¤í† ì–´."""

from __future__ import annotations

import json
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Sequence

import numpy as np
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from .data_loader import load_project_entries
from .text_chunker import split_into_chunks

DEFAULT_VECTORSTORE_PATH = Path("data/vectorstore.json")
EMBED_MODEL = os.environ.get("LANGGRAPH_EMBED_MODEL", "text-embedding-3-small")
CHUNK_SIZE = 800
CHUNK_OVERLAP = 200
CHROMA_DIR = (Path("store/chroma")).resolve()

@dataclass
class VectorChunk:
    id: str
    text: str
    metadata: Dict[str, str]
    embedding: List[float]


def _ensure_embeddings() -> OpenAIEmbeddings:
    load_dotenv(override=True)
    return OpenAIEmbeddings(model=EMBED_MODEL)

def _embed_in_batches(embeddings: OpenAIEmbeddings, texts: List[str], batch_size: int = 64) -> List[List[float]]:
    """
    OpenAI ì„ë² ë”©ì˜ ìš”ì²­ë‹¹ í† í° ìˆ˜ ì œí•œ(300k)ì„ í”¼í•˜ê¸° ìœ„í•´
    texts ë¥¼ ì—¬ëŸ¬ ë²ˆ ë‚˜ëˆ„ì–´ embed_documents ë¥¼ í˜¸ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    """
    all_vectors: List[List[float]] = []
    n = len(texts)

    for i in range(0, n, batch_size):
        batch = texts[i:i + batch_size]
        # í•„ìš”í•˜ë©´ ë””ë²„ê¹… ë¡œê·¸
        # print(f"Embedding batch {i} ~ {i + len(batch) - 1} / {n}")
        batch_vectors = embeddings.embed_documents(batch)
        all_vectors.extend(batch_vectors)

    return all_vectors

def build_chroma_store(chunks: List[VectorChunk]):
    """ê¸°ì¡´ JSON ë²¡í„°ìŠ¤í† ì–´ì™€ ë³„ê°œë¡œ Chromaì—ë„ ì €ì¥."""
    CHROMA_DIR.mkdir(parents=True, exist_ok=True)

    # HF / OpenAI ì–´ë–¤ ì„ë² ë”©ì´ë“ , ì´ë¯¸ chunk.embedding ì— ë“¤ì–´ìˆë‹¤ê³  ê°€ì •
    client = Chroma(
        collection_name="rfp_chunks",
        persist_directory=str(CHROMA_DIR),
        embedding_function=None,  # ìš°ë¦¬ëŠ” ì§ì ‘ ì„ë² ë”©í•´ì„œ ë„£ì„ê±°ë¼ None
    )

    # ê¸°ì¡´ ë°ì´í„° ë‚ ë¦¬ê³  ìƒˆë¡œ ì±„ìš°ê³  ì‹¶ìœ¼ë©´:
    client._collection.delete(where={})  # ì „ì²´ ì‚­ì œ (ì¡°ì‹¬!)

    client.add(
        ids=[c.id for c in chunks],
        documents=[c.text for c in chunks],
        metadatas=[c.metadata for c in chunks],
        embeddings=[c.embedding for c in chunks],
    )
    client.persist()
    return client

def build_vector_chunks() -> List[VectorChunk]:
    """ê° ì‚¬ì—… ì—”íŠ¸ë¦¬ë¥¼ VectorChunk ëª©ë¡ìœ¼ë¡œ ë‚˜ëˆˆë‹¤."""
    entries = load_project_entries()
    chunks: List[VectorChunk] = []
    for idx, entry in enumerate(entries):
        base_texts: List[str] = []
        summary = entry.get("summary")
        if summary:
            base_texts.append(summary.strip())
        full_text = entry.get("full_text") or entry.get("text_blob") or ""
        if full_text:
            base_texts.extend(split_into_chunks(full_text, CHUNK_SIZE, CHUNK_OVERLAP))
        if not base_texts:
            continue
        for part_idx, text in enumerate(base_texts):
            chunk_id = f"{idx:04d}-{part_idx:03d}"
            metadata = {
                "agency": entry.get("agency") or "",
                "project": entry.get("project") or "",
                "file_name": entry.get("file_name") or "",
                "source_index": str(idx),
            }
            chunks.append(VectorChunk(id=chunk_id, text=text, metadata=metadata, embedding=[]))
    return chunks


def create_vectorstore(output_path: Path = DEFAULT_VECTORSTORE_PATH) -> Path:
    """ì„ë² ë”©ì„ ìƒì„±í•´ JSON ë²¡í„°ìŠ¤í† ì–´ë¡œ ì €ì¥í•œë‹¤."""
    chunks = build_vector_chunks()
    if not chunks:
        raise ValueError("ìƒì„±í•  í…ìŠ¤íŠ¸ chunkê°€ ì—†ìŠµë‹ˆë‹¤.")

    embeddings = _ensure_embeddings()
    texts = [chunk.text for chunk in chunks]
    vectors = _embed_in_batches(embeddings, texts, batch_size=64)

    for chunk, vec in zip(chunks, vectors):
        chunk.embedding = vec

    payload = {
        "model": EMBED_MODEL,
        "dimension": len(chunks[0].embedding),
        "chunks": [
            {
                "id": chunk.id,
                "text": chunk.text,
                "metadata": chunk.metadata,
                "embedding": chunk.embedding,
            }
            for chunk in chunks
        ],
    }
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(payload, ensure_ascii=False), encoding="utf-8")

    # ğŸ”¥ ì„ íƒ: Chromaë„ í•¨ê»˜ êµ¬ì¶•
    try:
        build_chroma_store(chunks)
    except Exception as e:
        print(f"[WARN] Chroma ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

    return output_path

def load_vectorstore(path: Path = DEFAULT_VECTORSTORE_PATH):
    """JSON ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì½ì–´ numpy ê¸°ë°˜ ê²€ìƒ‰ìš© êµ¬ì¡°ë¡œ ë°˜í™˜í•œë‹¤."""
    if not path.exists():
        raise FileNotFoundError(f"{path} ë²¡í„°ìŠ¤í† ì–´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € build_vectorstore.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    data = json.loads(path.read_text(encoding="utf-8"))
    chunks = data.get("chunks", [])
    vectors = []
    texts = []
    metadata = []
    ids = []
    for chunk in chunks:
        ids.append(chunk.get("id"))
        texts.append(chunk.get("text", ""))
        metadata.append(chunk.get("metadata", {}))
        vec = chunk.get("embedding", [])
        vectors.append(vec)
    matrix = np.array(vectors, dtype=float)
    norms = np.linalg.norm(matrix, axis=1, keepdims=True)
    norms[norms == 0] = 1.0
    normalized = matrix / norms
    return {
        "ids": ids,
        "texts": texts,
        "metadata": metadata,
        "normalized": normalized,
    }


def _cosine_sim(query_vec: Sequence[float], doc_matrix: np.ndarray) -> np.ndarray:
    query = np.array(query_vec, dtype=float)
    norm = np.linalg.norm(query)
    if norm == 0:
        return np.zeros(doc_matrix.shape[0])
    query /= norm
    return doc_matrix @ query

def search_chroma(question: str, top_k: int = 3):
    """Chromaì— ì €ì¥ëœ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰."""
    embeddings = _ensure_embeddings()
    query_vec = embeddings.embed_query(question)

    client = Chroma(
        collection_name="rfp_chunks",
        persist_directory=str(CHROMA_DIR),
        embedding_function=None,
    )
    res = client._collection.query(
        query_embeddings=[query_vec],
        n_results=top_k,
    )

    results = []
    for i in range(len(res["ids"][0])):
        results.append(
            {
                "id": res["ids"][0][i],
                "text": res["documents"][0][i],
                "metadata": res["metadatas"][0][i],
                "score": float(res["distances"][0][i]) if "distances" in res else 0.0,
            }
        )
    return results



def search_vectorstore(question: str, store: dict, top_k: int = 3) -> List[Dict[str, str]]:
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ top_k ì²­í¬ë¥¼ ë°˜í™˜í•œë‹¤."""
    embeddings = _ensure_embeddings()
    query_vec = embeddings.embed_query(question)
    scores = _cosine_sim(query_vec, store["normalized"])
    top_indices = np.argsort(scores)[::-1][:top_k]
    results = []
    for idx in top_indices:
        results.append(
            {
                "id": store["ids"][idx],
                "text": store["texts"][idx],
                "metadata": store["metadata"][idx],
                "score": float(scores[idx]),
            }
        )
    return results
