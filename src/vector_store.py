"""OpenAI ì„ë² ë”©ì„ ì‚¬ìš©í•´ JSON íŒŒì¼ë¡œ ê´€ë¦¬í•˜ëŠ” ê°„ë‹¨í•œ ë²¡í„°ìŠ¤í† ì–´."""

from __future__ import annotations

import json
import os
import shutil
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Sequence

import numpy as np
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma

try:  # pragma: no cover
    from langchain_huggingface import HuggingFaceEmbeddings  # type: ignore
except Exception:  # pragma: no cover
    HuggingFaceEmbeddings = None  # type: ignore
from .data_loader import load_project_entries
from .text_chunker import split_into_chunks

DEFAULT_VECTORSTORE_PATH = Path("data/vectorstore.json")
EMBED_PROVIDER = os.environ.get("EMBED_PROVIDER", "openai").lower()  # openai | hf
EMBED_MODEL = os.environ.get("LANGGRAPH_EMBED_MODEL", "text-embedding-3-small")
HF_EMBED_MODEL = os.environ.get("HF_EMBED_MODEL", "BAAI/bge-m3")
CHUNK_SIZE = int(os.environ.get("CHUNK_SIZE", "1400"))
CHUNK_OVERLAP = int(os.environ.get("CHUNK_OVERLAP", "300"))
CHROMA_DIR = (Path("store/chroma")).resolve()

@dataclass
class VectorChunk:
    id: str
    text: str
    metadata: Dict[str, str]
    embedding: List[float]


def _ensure_embeddings():
    load_dotenv(override=True)
    if EMBED_PROVIDER == "hf" and HuggingFaceEmbeddings is not None:
        return HuggingFaceEmbeddings(model_name=HF_EMBED_MODEL)
    return OpenAIEmbeddings(model=EMBED_MODEL)

def _embed_in_batches(embeddings: OpenAIEmbeddings, texts: List[str], batch_size: int = 64) -> List[List[float]]:
    """
    OpenAI ì„ë² ë”©ì˜ ìš”ì²­ë‹¹ í† í° ìˆ˜ ì œí•œ(300k)ì„ í”¼í•˜ê¸° ìœ„í•´
    texts ë¥¼ ì—¬ëŸ¬ ë²ˆ ë‚˜ëˆ„ì–´ embed_documents ë¥¼ í˜¸ì¶œí•˜ëŠ” í—¬í¼ í•¨ìˆ˜
    """
    all_vectors: List[List[float]] = []
    n = len(texts)

    for i in range(0, n, batch_size):
        batch = texts[i:i + batch_size]
        # í•„ìš”í•˜ë©´ ë””ë²„ê¹… ë¡œê·¸
        # print(f"Embedding batch {i} ~ {i + len(batch) - 1} / {n}")
        batch_vectors = embeddings.embed_documents(batch)
        all_vectors.extend(batch_vectors)

    return all_vectors

def build_chroma_store(chunks: List[VectorChunk]):
    """ê¸°ì¡´ JSON ë²¡í„°ìŠ¤í† ì–´ì™€ ë³„ê°œë¡œ Chromaì—ë„ ì €ì¥."""
    # ë§¤ë²ˆ ìƒˆë¡œ êµ¬ì¶•í•  ë•Œ ì´ì „ ë°ì´í„°ê°€ ì„ì´ì§€ ì•Šë„ë¡ ì •ë¦¬
    if CHROMA_DIR.exists():
        shutil.rmtree(CHROMA_DIR, ignore_errors=True)
    CHROMA_DIR.mkdir(parents=True, exist_ok=True)

    # HF / OpenAI ì–´ë–¤ ì„ë² ë”©ì´ë“ , ì´ë¯¸ chunk.embedding ì— ë“¤ì–´ìˆë‹¤ê³  ê°€ì •
    client = Chroma(
        collection_name="rfp_chunks",
        persist_directory=str(CHROMA_DIR),
        embedding_function=None,  # ìš°ë¦¬ëŠ” ì§ì ‘ ì„ë² ë”©í•´ì„œ ë„£ì„ê±°ë¼ None
    )

    # langchain-chroma ìµœì‹  ë²„ì „ì—ì„œëŠ” add ëŒ€ì‹  ë‚´ë¶€ ì»¬ë ‰ì…˜ì— ì§ì ‘ ì¶”ê°€
    client._collection.add(
        ids=[c.id for c in chunks],
        documents=[c.text for c in chunks],
        metadatas=[c.metadata for c in chunks],
        embeddings=[c.embedding for c in chunks],
    )
    # Chroma ë²„ì „ì— ë”°ë¼ persist ìœ„ì¹˜ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆì–´ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
    if hasattr(client, "persist"):
        client.persist()
    elif hasattr(client, "_client") and hasattr(client._client, "persist"):
        client._client.persist()
    return client

def build_vector_chunks() -> List[VectorChunk]:
    """ê° ì‚¬ì—… ì—”íŠ¸ë¦¬ë¥¼ VectorChunk ëª©ë¡ìœ¼ë¡œ ë‚˜ëˆˆë‹¤."""
    entries = load_project_entries()
    chunks: List[VectorChunk] = []
    for idx, entry in enumerate(entries):
        base_texts: List[str] = []
        summary = entry.get("summary")
        if summary:
            base_texts.append(summary.strip())
        full_text = entry.get("full_text") or entry.get("text_blob") or ""
        if full_text:
            base_texts.extend(split_into_chunks(full_text, CHUNK_SIZE, CHUNK_OVERLAP))
        if not base_texts:
            continue
        for part_idx, text in enumerate(base_texts):
            chunk_id = f"{idx:04d}-{part_idx:03d}"
            metadata = {
                "agency": entry.get("agency") or "",
                "project": entry.get("project") or "",
                "file_name": entry.get("file_name") or "",
                "source_index": str(idx),
            }
            chunks.append(VectorChunk(id=chunk_id, text=text, metadata=metadata, embedding=[]))
    return chunks


def create_vectorstore(output_path: Path = DEFAULT_VECTORSTORE_PATH) -> Path:
    """ì„ë² ë”©ì„ ìƒì„±í•´ JSON ë²¡í„°ìŠ¤í† ì–´ë¡œ ì €ì¥í•œë‹¤."""
    # ê¸°ì¡´ ì‚°ì¶œë¬¼ ì •ë¦¬: JSON íŒŒì¼ ë° Chroma ë””ë ‰í„°ë¦¬
    if output_path.exists():
        output_path.unlink()
    chunks = build_vector_chunks()
    if not chunks:
        raise ValueError("ìƒì„±í•  í…ìŠ¤íŠ¸ chunkê°€ ì—†ìŠµë‹ˆë‹¤.")

    embeddings = _ensure_embeddings()
    texts = [chunk.text for chunk in chunks]
    vectors = _embed_in_batches(embeddings, texts, batch_size=64)

    for chunk, vec in zip(chunks, vectors):
        chunk.embedding = vec

    payload = {
        "model": EMBED_MODEL,
        "dimension": len(chunks[0].embedding),
        "chunks": [
            {
                "id": chunk.id,
                "text": chunk.text,
                "metadata": chunk.metadata,
                "embedding": chunk.embedding,
            }
            for chunk in chunks
        ],
    }
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(payload, ensure_ascii=False), encoding="utf-8")

    # ğŸ”¥ ì„ íƒ: Chromaë„ í•¨ê»˜ êµ¬ì¶•
    try:
        build_chroma_store(chunks)
    except Exception as e:
        print(f"[WARN] Chroma ë²¡í„°ìŠ¤í† ì–´ êµ¬ì¶• ì¤‘ ì—ëŸ¬ ë°œìƒ: {e}")

    return output_path

def load_vectorstore(path: Path = DEFAULT_VECTORSTORE_PATH):
    """JSON ë²¡í„°ìŠ¤í† ì–´ë¥¼ ì½ì–´ numpy ê¸°ë°˜ ê²€ìƒ‰ìš© êµ¬ì¡°ë¡œ ë°˜í™˜í•œë‹¤."""
    if not path.exists():
        raise FileNotFoundError(f"{path} ë²¡í„°ìŠ¤í† ì–´ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € build_vectorstore.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
    data = json.loads(path.read_text(encoding="utf-8"))
    chunks = data.get("chunks", [])
    vectors = []
    texts = []
    metadata = []
    ids = []
    for chunk in chunks:
        ids.append(chunk.get("id"))
        texts.append(chunk.get("text", ""))
        metadata.append(chunk.get("metadata", {}))
        vec = chunk.get("embedding", [])
        vectors.append(vec)
    matrix = np.array(vectors, dtype=float)
    norms = np.linalg.norm(matrix, axis=1, keepdims=True)
    norms[norms == 0] = 1.0
    normalized = matrix / norms
    return {
        "ids": ids,
        "texts": texts,
        "metadata": metadata,
        "normalized": normalized,
    }


def _cosine_sim(query_vec: Sequence[float], doc_matrix: np.ndarray) -> np.ndarray:
    query = np.array(query_vec, dtype=float)
    norm = np.linalg.norm(query)
    if norm == 0:
        return np.zeros(doc_matrix.shape[0])
    query /= norm
    return doc_matrix @ query


def _mmr(
    query_vec: Sequence[float],
    doc_matrix: np.ndarray,
    top_k: int,
    lambda_diversity: float = 0.5,
) -> List[int]:
    """ê°„ë‹¨í•œ MMRë¡œ ë‹¤ì–‘í•œ í›„ë³´ ì„ íƒ (doc_matrixëŠ” ì •ê·œí™”ëœ ë²¡í„°)."""
    if doc_matrix.size == 0:
        return []
    sims = _cosine_sim(query_vec, doc_matrix)
    selected = []
    candidates = set(range(doc_matrix.shape[0]))
    while candidates and len(selected) < top_k:
        if not selected:
            idx = int(np.argmax(sims[list(candidates)]))
            idx = list(candidates)[idx]
            selected.append(idx)
            candidates.remove(idx)
            continue
        mmr_scores = {}
        for idx in candidates:
            diversity = max(
                np.dot(doc_matrix[idx], doc_matrix[j]) for j in selected
            )
            mmr_scores[idx] = lambda_diversity * sims[idx] - (1 - lambda_diversity) * diversity
        best = max(mmr_scores, key=mmr_scores.get)
        selected.append(best)
        candidates.remove(best)
    return selected

def search_chroma(question: str, top_k: int = 3):
    """Chromaì— ì €ì¥ëœ ë²¡í„°ìŠ¤í† ì–´ì—ì„œ ê²€ìƒ‰."""
    embeddings = _ensure_embeddings()
    query_vec = embeddings.embed_query(question)

    client = Chroma(
        collection_name="rfp_chunks",
        persist_directory=str(CHROMA_DIR),
        embedding_function=None,
    )
    res = client._collection.query(
        query_embeddings=[query_vec],
        n_results=top_k,
    )

    results = []
    for i in range(len(res["ids"][0])):
        results.append(
            {
                "id": res["ids"][0][i],
                "text": res["documents"][0][i],
                "metadata": res["metadatas"][0][i],
                "score": float(res["distances"][0][i]) if "distances" in res else 0.0,
            }
        )
    return results



def search_vectorstore(question: str, store: dict, top_k: int = 3) -> List[Dict[str, str]]:
    """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ top_k ì²­í¬ë¥¼ ë°˜í™˜í•œë‹¤."""
    enable_mmr = os.environ.get("ENABLE_MMR", "0").lower() not in {"0", "false", "no"}
    mmr_lambda = float(os.environ.get("MMR_LAMBDA", "0.5"))
    embeddings = _ensure_embeddings()
    query_vec = embeddings.embed_query(question)
    scores = _cosine_sim(query_vec, store["normalized"])
    if enable_mmr:
        top_indices = _mmr(query_vec, store["normalized"], top_k=top_k, lambda_diversity=mmr_lambda)
    else:
        top_indices = np.argsort(scores)[::-1][:top_k]
    results = []
    for idx in top_indices:
        results.append(
            {
                "id": store["ids"][idx],
                "text": store["texts"][idx],
                "metadata": store["metadata"][idx],
                "score": float(scores[idx]),
            }
        )
    return results
