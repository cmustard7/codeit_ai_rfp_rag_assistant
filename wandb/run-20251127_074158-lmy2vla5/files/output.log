[34m[1mwandb[0m: Detected [huggingface_hub.inference, openai] in use.
[34m[1mwandb[0m: Use W&B Weave for improved LLM call tracing. Install Weave with `pip install weave` then add `import weave` to the top of your script.
[34m[1mwandb[0m: For more information, check out the docs at: https://weave-docs.wandb.ai/
Traceback (most recent call last):
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 402, in hf_raise_for_status
    response.raise_for_status()
  File "/opt/jhub-venv/lib/python3.12/site-packages/requests/models.py", line 1026, in raise_for_status
    raise HTTPError(http_error_msg, response=self)
requests.exceptions.HTTPError: 422 Client Error: Unprocessable Entity for url: http://127.0.0.1:8080/

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py", line 2368, in text_generation
    bytes_output = self._inner_post(request_parameters, stream=stream or False)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py", line 275, in _inner_post
    hf_raise_for_status(response)
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/huggingface_hub/utils/_http.py", line 475, in hf_raise_for_status
    raise _format(HfHubHTTPError, str(e), response) from e
huggingface_hub.errors.HfHubHTTPError: 422 Client Error: Unprocessable Entity for url: http://127.0.0.1:8080/

Input validation error: `inputs` must have less than 4096 tokens. Given: 4324
{"error":"Input validation error: `inputs` must have less than 4096 tokens. Given: 4324","error_type":"validation"}


The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/home/minhyuk/codeit_ai_rfp_rag_assistant/src/run_eval.py", line 173, in <module>
    main()
  File "/home/minhyuk/codeit_ai_rfp_rag_assistant/src/run_eval.py", line 122, in main
    state = workflow.invoke(state)
            ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/langgraph/pregel/main.py", line 3068, in invoke
    for chunk in self.stream(
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/langgraph/pregel/main.py", line 2643, in stream
    for _ in runner.tick(
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/langgraph/pregel/_runner.py", line 167, in tick
    run_with_retry(
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/langgraph/pregel/_retry.py", line 42, in run_with_retry
    return task.proc.invoke(task.input, config)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py", line 656, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py", line 400, in invoke
    ret = self.func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/minhyuk/codeit_ai_rfp_rag_assistant/src/nodes/answer.py", line 63, in answer_question
    response = _llm.invoke(prompt)
               ^^^^^^^^^^^^^^^^^^^
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py", line 373, in invoke
    self.generate_prompt(
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py", line 784, in generate_prompt
    return self.generate(prompt_strings, stop=stop, callbacks=callbacks, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py", line 1006, in generate
    return self._generate_helper(
           ^^^^^^^^^^^^^^^^^^^^^^
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py", line 810, in _generate_helper
    self._generate(
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/langchain_core/language_models/llms.py", line 1500, in _generate
    self._call(prompt, stop=stop, run_manager=run_manager, **kwargs)
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/langchain_huggingface/llms/huggingface_endpoint.py", line 341, in _call
    response_text = self.client.text_generation(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/huggingface_hub/inference/_client.py", line 2398, in text_generation
    raise_text_generation_error(e)
  File "/home/minhyuk/myenv/lib/python3.12/site-packages/huggingface_hub/inference/_common.py", line 444, in raise_text_generation_error
    raise exception from http_error
huggingface_hub.errors.ValidationError: Input validation error: `inputs` must have less than 4096 tokens. Given: 4324
During task with name 'answer' and id '108b8292-4d82-9bfe-482a-89eb407e8e78'
