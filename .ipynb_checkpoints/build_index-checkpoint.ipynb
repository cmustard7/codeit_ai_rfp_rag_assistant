{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, zipfile, subprocess, time, unicodedata\n",
    "from pathlib import Path\n",
    "from difflib import get_close_matches\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "import fitz\n",
    "import tiktoken\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 경로/설정 ======\n",
    "DATA_DIR = Path(\"data\")\n",
    "FILES_DIR = DATA_DIR / \"files\"              # 원본 파일 저장 폴더\n",
    "LIST_XLSX = DATA_DIR / \"data_list.xlsx\"\n",
    "LIST_CSV  = DATA_DIR / \"data_list.csv\"\n",
    "CHROMA_DIR = (Path(\"store/chroma\")).resolve(); CHROMA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "EMBED_BACKEND = \"openai\"   # \"openai\" | \"bge\"\n",
    "OPENAI_EMBED_MODEL = \"text-embedding-3-small\"\n",
    "BGE_MODEL = \"BAAI/bge-m3\"\n",
    "\n",
    "# 한국어 문서 기준 무난한 값 (너무 짧으면 문맥 끊김, 너무 길면 검색 정밀도↓/속도↓)\n",
    "CHUNK_CHARS = 1200\n",
    "CHUNK_OVERLAP = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노트북/스크립트 어디서 실행하든 .env를 찾도록\n",
    "HERE = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "env_path = (HERE / \".env\")\n",
    "if env_path.exists():\n",
    "    load_dotenv(dotenv_path=env_path, override=False)\n",
    "else:\n",
    "    # CWD 기준 탐색도 시도\n",
    "    load_dotenv(find_dotenv(usecwd=True), override=False)\n",
    "\n",
    "# 런타임에 실제로 잡혔는지 강제 확인(없으면 바로 오류)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key or not api_key.startswith(\"sk-\"):\n",
    "    raise RuntimeError(\"[ENV] OPENAI_API_KEY가 런타임에 설정되지 않았습니다. .env 위치/CWD/커널 확인 필요.\")\n",
    "print(\"[ENV] OPENAI_API_KEY loaded:\", api_key[:10] + \"…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 파일명 정규화/매칭 ======\n",
    "def canon_filename(name: str | None) -> str:\n",
    "    \"\"\"파일명 매칭 표준화: NFC, 숨은공백 제거, 하이픈/괄호 통일, 공백축약, 확장자 소문자\"\"\"\n",
    "    if not name:\n",
    "        return \"\"\n",
    "    s = unicodedata.normalize(\"NFC\", str(name))\n",
    "    s = (s.replace(\"\\uFEFF\",\"\")\n",
    "           .replace(\"\\u200B\",\"\")\n",
    "           .replace(\"\\u00A0\",\" \"))\n",
    "    # 각종 대쉬/괄호 통일\n",
    "    s = (s.replace(\"–\",\"-\").replace(\"—\",\"-\").replace(\"−\",\"-\")\n",
    "           .replace(\"（\",\"(\").replace(\"）\",\")\"))\n",
    "    s = re.sub(r\"\\s+\",\" \", s.strip())\n",
    "    p = Path(s)\n",
    "    return p.stem + p.suffix.lower()\n",
    "\n",
    "def build_files_index(root: Path) -> dict[str, Path]:\n",
    "    \"\"\"하위폴더까지 모두 인덱싱\"\"\"\n",
    "    idx: dict[str, Path] = {}\n",
    "    for p in root.rglob(\"*\"):\n",
    "        if p.is_file():\n",
    "            idx[canon_filename(p.name)] = p\n",
    "    return idx\n",
    "\n",
    "def find_path_by_filename(files_index: dict[str, Path], raw: str | None) -> Path | None:\n",
    "    if not raw:\n",
    "        return None\n",
    "    key = canon_filename(raw)\n",
    "    # 1) 완전 일치\n",
    "    if key in files_index:\n",
    "        return files_index[key]\n",
    "    # 2) 확장자 제외 스템 일치\n",
    "    stem = Path(key).stem\n",
    "    cand = [k for k in files_index if Path(k).stem == stem]\n",
    "    if cand:\n",
    "        return files_index[cand[0]]\n",
    "    # 3) 근사 매칭\n",
    "    close = get_close_matches(key, list(files_index), n=1, cutoff=0.88)\n",
    "    return files_index[close[0]] if close else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 텍스트 유틸 ======\n",
    "def clean_text(s: str) -> str:\n",
    "    if not isinstance(s, str):\n",
    "        s = \"\" if pd.isna(s) else str(s)\n",
    "    s = s.replace(\"\\x00\", \"\")\n",
    "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s.strip())\n",
    "    return s\n",
    "\n",
    "def maybe_ocr_needed(text: str) -> bool:\n",
    "    letters = re.findall(r\"[A-Za-z가-힣]\", text or \"\")\n",
    "    return len(letters) < 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 파일별 텍스트 추출 ======\n",
    "def pdf_to_text(pdf_path: Path) -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages = [p.get_text(\"text\") for p in doc]\n",
    "    txt = \"\\n\\n\".join(pages)\n",
    "    if maybe_ocr_needed(txt):\n",
    "        try:\n",
    "            ocrd = pdf_path.with_suffix(\".ocr.pdf\")\n",
    "            subprocess.run([\"ocrmypdf\", \"--force-ocr\", \"--skip-text\", str(pdf_path), str(ocrd)], check=True)\n",
    "            doc = fitz.open(ocrd)\n",
    "            pages = [p.get_text(\"text\") for p in doc]\n",
    "            txt = \"\\n\\n\".join(pages)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] OCR 실패: {pdf_path.name} - {e}\")\n",
    "    return clean_text(txt)\n",
    "\n",
    "def hwpx_to_text(hwpx_path: Path) -> str:\n",
    "    texts = []\n",
    "    with zipfile.ZipFile(hwpx_path) as zf:\n",
    "        for name in zf.namelist():\n",
    "            if name.startswith(\"Contents/\") and name.endswith(\".xml\"):\n",
    "                try:\n",
    "                    xml = zf.read(name)\n",
    "                    root = etree.fromstring(xml)\n",
    "                    for node in root.iter():\n",
    "                        if node.tag.endswith(\"p\"):\n",
    "                            t = \"\".join(node.itertext()).strip()\n",
    "                            if t:\n",
    "                                texts.append(t)\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] HWPX XML 파싱 실패: {name} - {e}\")\n",
    "    return clean_text(\"\\n\".join(texts))\n",
    "\n",
    "def hwp_to_text(hwp_path: Path) -> str:\n",
    "    try:\n",
    "        out = subprocess.check_output([\"hwp5txt\", str(hwp_path)], text=True)\n",
    "        return clean_text(out)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] hwp5txt 실패: {hwp_path.name} - {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_text_by_format(path: Path, fmt: str) -> str:\n",
    "    fmt = (fmt or path.suffix).lower().strip(\".\")\n",
    "    if fmt == \"pdf\":\n",
    "        return pdf_to_text(path)\n",
    "    if fmt == \"hwpx\":\n",
    "        return hwpx_to_text(path)\n",
    "    if fmt == \"hwp\":\n",
    "        return hwp_to_text(path)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 임베딩 백엔드 ======\n",
    "def get_embedder():\n",
    "    if EMBED_BACKEND == \"openai\":\n",
    "        # ✅ batch_size 사용, 재시도 강화\n",
    "        return OpenAIEmbeddings(model=OPENAI_EMBED_MODEL, max_retries=8, api_key=api_key)\n",
    "    else:\n",
    "        class STEmb:\n",
    "            def __init__(self, model_name):\n",
    "                self.model = SentenceTransformer(model_name)\n",
    "            def embed_documents(self, texts: List[str]):\n",
    "                return self.model.encode(texts, batch_size=16, normalize_embeddings=True).tolist()\n",
    "            def embed_query(self, text: str):\n",
    "                return self.model.encode([text], normalize_embeddings=True).tolist()[0]\n",
    "        return STEmb(BGE_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 메타데이터 적재 ======\n",
    "def load_data_list() -> pd.DataFrame:\n",
    "    if LIST_XLSX.exists():\n",
    "        df_x = pd.read_excel(LIST_XLSX)\n",
    "    else:\n",
    "        df_x = pd.DataFrame()\n",
    "    if LIST_CSV.exists():\n",
    "        df_c = pd.read_csv(LIST_CSV)\n",
    "    else:\n",
    "        df_c = pd.DataFrame()\n",
    "\n",
    "    if not df_x.empty and not df_c.empty:\n",
    "        key = \"파일명\" if \"파일명\" in df_x.columns and \"파일명\" in df_c.columns else None\n",
    "        if key:\n",
    "            df = df_c.set_index(key).combine_first(df_x.set_index(key)).reset_index()\n",
    "        else:\n",
    "            df = pd.concat([df_x, df_c]).drop_duplicates()\n",
    "    else:\n",
    "        df = df_x if not df_x.empty else df_c\n",
    "\n",
    "    cols = {\n",
    "        \"공고 번호\": \"notice_id\",\n",
    "        \"공고 차수\": \"round\",\n",
    "        \"사업명\": \"project_name\",\n",
    "        \"사업 금액\": \"budget\",\n",
    "        \"발주 기관\": \"agency\",\n",
    "        \"공개 일자\": \"published_at\",\n",
    "        \"입찰 참여 시작일\": \"bid_start\",\n",
    "        \"입찰 참여 마감일\": \"bid_end\",\n",
    "        \"사업 요약\": \"summary\",\n",
    "        \"파일형식\": \"file_format\",\n",
    "        \"파일명\": \"filename\",\n",
    "        \"텍스트\": \"text\",\n",
    "    }\n",
    "    df = df.rename(columns={k: v for k, v in cols.items() if k in df.columns})\n",
    "    for c in [\"notice_id\",\"round\",\"project_name\",\"budget\",\"agency\",\"published_at\",\n",
    "              \"bid_start\",\"bid_end\",\"summary\",\"file_format\",\"filename\",\"text\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].apply(clean_text)\n",
    "        else:\n",
    "            df[c] = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Document 생성 ======\n",
    "def row_to_document(row: pd.Series, files_index: dict[str, Path]) -> Document:\n",
    "    filename = row[\"filename\"]\n",
    "    fmt = row[\"file_format\"] or (Path(filename).suffix.strip(\".\") if filename else None)\n",
    "    text = row[\"text\"]\n",
    "\n",
    "    if not text:\n",
    "        path = find_path_by_filename(files_index, filename)\n",
    "        if path:\n",
    "            text = extract_text_by_format(path, fmt or path.suffix.strip(\".\"))\n",
    "        else:\n",
    "            print(f\"[MISS] files/에서 못 찾음 -> {filename!r}\")\n",
    "            text = \"\"\n",
    "\n",
    "    meta = {\n",
    "        \"filename\": filename,\n",
    "        \"notice_id\": row[\"notice_id\"],\n",
    "        \"round\": row[\"round\"],\n",
    "        \"project_name\": row[\"project_name\"],\n",
    "        \"budget\": row[\"budget\"],\n",
    "        \"agency\": row[\"agency\"],\n",
    "        \"published_at\": row[\"published_at\"],\n",
    "        \"bid_start\": row[\"bid_start\"],\n",
    "        \"bid_end\": row[\"bid_end\"],\n",
    "        \"file_format\": fmt,\n",
    "    }\n",
    "    return Document(page_content=text, metadata=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    df = load_data_list()\n",
    "    if df.empty:\n",
    "        print(\"[ERROR] data_list가 비어있습니다.\")\n",
    "        return\n",
    "\n",
    "    # files 인덱스 구축 (하위폴더 포함)\n",
    "    files_index = build_files_index(FILES_DIR)\n",
    "\n",
    "    # 미스 리포트\n",
    "    miss = []\n",
    "    for fn in df[\"filename\"]:\n",
    "        if not fn:\n",
    "            continue\n",
    "        if find_path_by_filename(files_index, fn) is None:\n",
    "            miss.append(fn)\n",
    "    if miss:\n",
    "        print(f\"[WARN] files/에 없는 파일 {len(miss)}건 (예: {miss[:5]})\")\n",
    "        # 추가 리포트 파일 생성\n",
    "        rows = []\n",
    "        for fn in miss:\n",
    "            key = canon_filename(fn)\n",
    "            cand = get_close_matches(key, list(files_index), n=1, cutoff=0.7)\n",
    "            rows.append({\"원본파일명\": fn, \"정규화후\": key, \"근사후보\": cand[0] if cand else \"\"})\n",
    "        if rows:\n",
    "            pd.DataFrame(rows).to_csv(\"missing_files_report.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "            print(\"[REPORT] missing_files_report.csv 생성\")\n",
    "\n",
    "    # Document 리스트 생성\n",
    "    docs: List[Document] = []\n",
    "    for _, row in df.iterrows():\n",
    "        doc = row_to_document(row, files_index)\n",
    "        if doc.page_content:\n",
    "            docs.append(doc)\n",
    "\n",
    "    if not docs:\n",
    "        print(\"[ERROR] 텍스트가 비어 문서가 없습니다. 추출 파이프라인/파일명 확인 필요.\")\n",
    "        return\n",
    "\n",
    "    # 청킹\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_CHARS,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        separators=[\"\\n제 \", \"\\n## \", \"\\n### \", \"\\n\", \" \", \"\"],\n",
    "    )\n",
    "    chunks = splitter.split_documents(docs)\n",
    "    print(f\"[INFO] total docs: {len(docs)}, chunks: {len(chunks)}\")\n",
    "\n",
    "    # 임베딩 & Chroma 인덱스\n",
    "    embeddings = get_embedder()\n",
    "    vectordb = Chroma(\n",
    "        collection_name=\"bidmate_rag\",\n",
    "        embedding_function=embeddings,\n",
    "        persist_directory=str(CHROMA_DIR),\n",
    "    )\n",
    "\n",
    "    # 이미 들어간 ID 스킵(재실행 이어붙기) — 선택적 최적화\n",
    "    existing_ids = set()\n",
    "    try:\n",
    "        got = vectordb._collection.get(include=[], limit=0)  # 일부 드라이버에서 limit=0 허용X면 제외\n",
    "        existing_ids = set(got.get(\"ids\", []))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 토큰 길이 측정기\n",
    "    enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    def tok_len(s: str) -> int:\n",
    "        return len(enc.encode(s or \"\"))\n",
    "\n",
    "    # 안전한 배치: 토큰 예산 + 개수 상한 + 재시도(backoff)\n",
    "    TOKEN_BUDGET = 150_000\n",
    "    MAX_ITEMS    = 8\n",
    "\n",
    "    texts  = [c.page_content for c in chunks]\n",
    "    metas  = [c.metadata      for c in chunks]\n",
    "    # 파일명+인덱스 기반 고유 ID\n",
    "    ids    = [f\"{canon_filename(m.get('filename') or 'no-file')}::{i:08d}\" for i, m in enumerate(metas)]\n",
    "\n",
    "    # 기존 ID 제외\n",
    "    if existing_ids:\n",
    "        filtered = [(t,m,i) for t,m,i in zip(texts, metas, ids) if i not in existing_ids]\n",
    "        texts, metas, ids = (list(x) for x in zip(*filtered)) if filtered else ([],[],[])\n",
    "        print(f\"[INFO] skipping existing ids, to_add: {len(texts)}\")\n",
    "\n",
    "    i, n = 0, len(texts)\n",
    "    while i < n:\n",
    "        budget, j = 0, i\n",
    "        while j < n and (j - i) < MAX_ITEMS:\n",
    "            tlen = tok_len(texts[j])\n",
    "            if tlen > 20000:\n",
    "                # 방어적 자르기 (권장: 청킹값을 더 줄이는 게 근본 해결)\n",
    "                texts[j] = texts[j][:8000]\n",
    "                tlen = tok_len(texts[j])\n",
    "            if budget + tlen > TOKEN_BUDGET:\n",
    "                break\n",
    "            budget += tlen\n",
    "            j += 1\n",
    "\n",
    "        t_batch, m_batch, id_batch = texts[i:j], metas[i:j], ids[i:j]\n",
    "\n",
    "        for attempt in range(1, 9):  # 최대 8회 백오프\n",
    "            try:\n",
    "                vecs = embeddings.embed_documents(t_batch)\n",
    "                vectordb._collection.add(\n",
    "                    embeddings=vecs,\n",
    "                    documents=t_batch,\n",
    "                    metadatas=m_batch,\n",
    "                    ids=id_batch,\n",
    "                )\n",
    "                print(f\"  -> indexed {j}/{n} (batch tokens~{budget})\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                wait = min(30, 2 ** attempt)\n",
    "                print(f\"[WARN] embed batch failed (try {attempt}): {e} -> sleep {wait}s\")\n",
    "                time.sleep(wait)\n",
    "\n",
    "        i = j\n",
    "\n",
    "    if hasattr(vectordb, \"persist\"):\n",
    "        vectordb.persist()\n",
    "    elif hasattr(vectordb._client, \"persist\"):\n",
    "        vectordb._client.persist()\n",
    "    print(f\"✅ Indexed {n} chunks into {CHROMA_DIR}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
