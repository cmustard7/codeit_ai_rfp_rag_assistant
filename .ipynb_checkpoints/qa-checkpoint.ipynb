{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, zipfile, subprocess\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any\n",
    "import pandas as pd\n",
    "from lxml import etree\n",
    "import fitz\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from sentence_transformers import SentenceTransformer \n",
    "from langchain_chroma import Chroma\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_DIR = \"store/chroma\"\n",
    "EMBED_BACKEND = \"openai\"   # \"openai\" | \"bge\"\n",
    "OPENAI_EMBED_MODEL = \"text-embedding-3-small\"\n",
    "BGE_MODEL = \"BAAI/bge-m3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 노트북/스크립트 어디서 실행하든 .env를 찾도록\n",
    "HERE = Path(__file__).resolve().parent if \"__file__\" in globals() else Path.cwd()\n",
    "env_path = (HERE / \".env\")\n",
    "if env_path.exists():\n",
    "    load_dotenv(dotenv_path=env_path, override=False)\n",
    "else:\n",
    "    # CWD 기준 탐색도 시도\n",
    "    load_dotenv(find_dotenv(usecwd=True), override=False)\n",
    "\n",
    "# 런타임에 실제로 잡혔는지 강제 확인(없으면 바로 오류)\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not api_key or not api_key.startswith(\"sk-\"):\n",
    "    raise RuntimeError(\"[ENV] OPENAI_API_KEY가 런타임에 설정되지 않았습니다. .env 위치/CWD/커널 확인 필요.\")\n",
    "print(\"[ENV] OPENAI_API_KEY loaded:\", api_key[:10] + \"…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedder():\n",
    "    if EMBED_BACKEND == \"openai\":\n",
    "        return OpenAIEmbeddings(\n",
    "            model=OPENAI_EMBED_MODEL,\n",
    "            api_key=api_key, \n",
    "            max_retries=8,\n",
    "            request_timeout=60,\n",
    "        )\n",
    "    else:\n",
    "        class STEmb:\n",
    "            def __init__(self, name): self.m = SentenceTransformer(name)\n",
    "            def embed_documents(self, ts): return self.m.encode(ts, batch_size=32, normalize_embeddings=True).tolist()\n",
    "            def embed_query(self, t): return self.m.encode([t], normalize_embeddings=True).tolist()[0]\n",
    "        return STEmb(BGE_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_retriever(filters: Dict[str, Any] | None = None, k: int = 6):\n",
    "    emb = get_embedder()\n",
    "    db = Chroma(collection_name=\"bidmate_rag\", embedding_function=emb, persist_directory=CHROMA_DIR)\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": k})\n",
    "    # Chroma의 where 필터는 VectorStore.query에서 사용. LangChain retriever에선 직접 filter 전달 가능\n",
    "    if filters:\n",
    "        retriever.search_kwargs[\"filter\"] = filters\n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-5-mini\", temperature=0, api_key=api_key)\n",
    "\n",
    "    # 예시 필터: 특정 기관 + 파일형식(pdf만)\n",
    "    # filters = {\"agency\": \"한국전력공사\", \"file_format\": \"pdf\"}\n",
    "\n",
    "    emb = get_embedder()  # OpenAIEmbeddings 또는 bge STEmb\n",
    "    db = Chroma(collection_name=\"bidmate_rag\", embedding_function=emb, persist_directory=CHROMA_DIR)\n",
    "\n",
    "    retriever = db.as_retriever(search_kwargs={\"k\": 6})\n",
    "\n",
    "    while True:\n",
    "        q = input(\"\\nQ> \").strip()\n",
    "        if not q:\n",
    "            break\n",
    "        docs = retriever.invoke(q)\n",
    "\n",
    "        context = \"\"\n",
    "        used = set()\n",
    "        for d in docs[:5]:\n",
    "            fn = d.metadata.get(\"filename\")\n",
    "            tag = f\"[{fn}|{d.metadata.get('agency')}|{d.metadata.get('published_at')}]\"\n",
    "            if fn in used:  # 중복 방지\n",
    "                continue\n",
    "            used.add(fn)\n",
    "            context += f\"{tag}\\n{d.page_content[:1200]}\\n\\n\"\n",
    "\n",
    "        prompt = (\n",
    "            \"다음 컨텍스트만 근거로 한국어로 정확하고 간결하게 답하세요. \"\n",
    "            \"반드시 근거로 사용한 출처를 대괄호로 표시하세요.\\n\\n\"\n",
    "            f\"{context}\\n질문: {q}\\n\\n답변:\"\n",
    "        )\n",
    "        ans = llm.invoke(prompt).content\n",
    "        print(\"\\nA>\", ans)\n",
    "        print(\"\\n[SOURCES]\")\n",
    "        for d in docs[:5]:\n",
    "            print(\"-\", d.metadata.get(\"filename\"), d.metadata.get(\"agency\"), d.metadata.get(\"published_at\"))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
